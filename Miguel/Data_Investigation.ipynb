{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import h5py\n",
    "import numpy as np\n",
    "import awkward1 as ak\n",
    "import pandas as pd\n",
    "import mplhep as hep\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/eos/home-m/malvesga/SWAN_projects/output_AQAg/\" # Caminho comum para todos os arquivos .h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função que abre os arquivos .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file( file ):\n",
    "    df = None\n",
    "    with h5py.File( file , 'r' ) as f:\n",
    "        #print ( list(f.keys()) )\n",
    "        dset = f['dados']\n",
    "        #print ( dset.shape )\n",
    "        #print ( dset[:,:] )\n",
    "        dset_columns = f['columns']\n",
    "        #print ( dset_columns.shape )\n",
    "        columns_ = list( dset_columns )\n",
    "        #print ( columns_ )\n",
    "        columns_str = [ item.decode(\"utf-8\") for item in columns_ ]\n",
    "        #print ( columns_str )\n",
    "        df = pd.DataFrame( dset, columns = columns_str )\n",
    "        array = np.array( dset )\n",
    "        #print ( df )    \n",
    "        return array\n",
    "        #return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luminosidade Integrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleMuon_Run2016B = 4.55\n",
    "SingleMuon_Run2016C = 1.59\n",
    "SingleMuon_Run2016G = 3.65\n",
    "Luminosidade        = SingleMuon_Run2016B + SingleMuon_Run2016C + SingleMuon_Run2016G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amostras de Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Normalização "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seção de Choque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_SM       = 40.41*0.17\n",
    "cross_section_ANOMALO1 = 166.1*0.17 \n",
    "cross_section_ANOMALO2 = 41.90*0.17\n",
    "cross_section_ANOMALO3 = 48.75*0.17\n",
    "cross_section_ANOMALO4 = 61.14*0.17\n",
    "cross_section_ANOMALO5 = 41.58*0.17\n",
    "cross_section_ANOMALO6 = 44.93*0.17\n",
    "cross_section_ANOMALO7 = 58.18*0.17 \n",
    "cross_section_ANOMALO8 = 150.3*0.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de Eventos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_events_SM       = 35000\n",
    "number_events_ANOMALO1 = 35000\n",
    "number_events_ANOMALO2 = 35000\n",
    "number_events_ANOMALO3 = 35000\n",
    "number_events_ANOMALO4 = 35000\n",
    "number_events_ANOMALO5 = 35000\n",
    "number_events_ANOMALO6 = 35000\n",
    "number_events_ANOMALO7 = 35000\n",
    "number_events_ANOMALO8 = 35000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacao dos eventos de SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_SM = ( cross_section_SM * Luminosidade ) / ( number_events_SM )\n",
    "norm_ANOMALO1 = ( cross_section_ANOMALO1 * Luminosidade ) / ( number_events_ANOMALO1 )\n",
    "norm_ANOMALO2 = ( cross_section_ANOMALO2 * Luminosidade ) / ( number_events_ANOMALO2 )\n",
    "norm_ANOMALO3 = ( cross_section_ANOMALO3 * Luminosidade ) / ( number_events_ANOMALO3 )\n",
    "norm_ANOMALO4 = ( cross_section_ANOMALO4 * Luminosidade ) / ( number_events_ANOMALO4 )\n",
    "norm_ANOMALO5 = ( cross_section_ANOMALO5 * Luminosidade ) / ( number_events_ANOMALO5 )\n",
    "norm_ANOMALO6 = ( cross_section_ANOMALO6 * Luminosidade ) / ( number_events_ANOMALO6 )\n",
    "norm_ANOMALO7 = ( cross_section_ANOMALO7 * Luminosidade ) / ( number_events_ANOMALO7 )\n",
    "norm_ANOMALO8 = ( cross_section_ANOMALO8 * Luminosidade ) / ( number_events_ANOMALO8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Importando as amostras de Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM = PATH + 'output-SM.h5'\n",
    "ANOMALO1 = PATH + 'output-ANOMALO1.h5'\n",
    "ANOMALO2 = PATH + 'output-ANOMALO2.h5'\n",
    "ANOMALO3 = PATH + 'output-ANOMALO3.h5'\n",
    "ANOMALO4 = PATH + 'output-ANOMALO4.h5'\n",
    "ANOMALO5 = PATH + 'output-ANOMALO5.h5'\n",
    "ANOMALO6 = PATH + 'output-ANOMALO6.h5'\n",
    "ANOMALO7 = PATH + 'output-ANOMALO7.h5'\n",
    "ANOMALO8 = PATH + 'output-ANOMALO8.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM =       open_file(SM)\n",
    "ANOMALO1 = open_file(ANOMALO1)\n",
    "ANOMALO2 = open_file(ANOMALO2)\n",
    "ANOMALO3 = open_file(ANOMALO3)\n",
    "ANOMALO4 = open_file(ANOMALO4)\n",
    "ANOMALO5 = open_file(ANOMALO5)\n",
    "ANOMALO6 = open_file(ANOMALO6)\n",
    "ANOMALO7 = open_file(ANOMALO7)\n",
    "ANOMALO8 = open_file(ANOMALO8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amostras de Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Normalização "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seção de Choque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section_TT = 831.7\n",
    "cross_section_inclusive_WZ = 10.73\n",
    "cross_section_inclusive_WW = 49.997\n",
    "cross_section_inclusive_ZZ = 3.28\n",
    "cross_section_ST_s_channel = 3.365\n",
    "cross_section_ST_t_channel_top = 136.02\n",
    "cross_section_ST_t_channel_antitop = 80.95\n",
    "cross_section_ST_tW_top= 35.85\n",
    "cross_section_ST_tW_antitop = 35.85\n",
    "cross_section_DYJetsToLL_Pt_100To250 = 83.12\n",
    "cross_section_DYJetsToLL_Pt_250To400 = 3.047\n",
    "cross_section_DYJetsToLL_Pt_400To650 = 0.3921\n",
    "cross_section_DYJetsToLL_Pt_650ToInf = 0.0363\n",
    "cross_section_QCD_Pt_170to300 = 8654\n",
    "cross_section_QCD_Pt_300to470 = 797.3\n",
    "cross_section_QCD_Pt_470to600 = 79.0\n",
    "cross_section_QCD_Pt_600to800 = 25.09\n",
    "cross_section_QCD_Pt_800to1000 = 4.7\n",
    "cross_section_QCD_Pt_1000toInf = 1.6\n",
    "cross_section_WJetsToLNu_Pt_100To250 = 677.82\n",
    "cross_section_WJetsToLNu_Pt_250To400 = 24.083\n",
    "cross_section_WJetsToLNu_Pt_400To600 = 3.0563\n",
    "cross_section_WJetsToLNu_Pt_600ToInf = 0.4602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_events_TT = 76915549\n",
    "number_events_inclusive_WZ = 24311445\n",
    "number_events_inclusive_WW = 6655400 + 1999200\n",
    "number_events_inclusive_ZZ = 15061141 + 755866\n",
    "number_events_ST_s_channel = 1000000\n",
    "number_events_ST_t_channel_top = 43864048\n",
    "number_events_ST_t_channel_antitop = 38811017\n",
    "number_events_ST_tW_top = 6952830\n",
    "number_events_ST_tW_antitop = 6933094\n",
    "number_events_DYJetsToLL_Pt_100To250 =  2991815 + 2805972 + 2046961\n",
    "number_events_DYJetsToLL_Pt_250To400 = 47559302 + 594317 + 590806 + 423976\n",
    "number_events_DYJetsToLL_Pt_400To650 = 604038 + 589842 + 432056\n",
    "number_events_DYJetsToLL_Pt_650ToInf = 597526 + 430691\n",
    "number_events_QCD_Pt_170to300 = 19789673 + 7947159\n",
    "number_events_QCD_Pt_300to470 = 24605508 + 16462878 + 7937590\n",
    "number_events_QCD_Pt_470to600 = 9847664 + 5668793 + 3972819\n",
    "number_events_QCD_Pt_600to800 = 9928218 + 5971175 + 401013\n",
    "number_events_QCD_Pt_800to1000 = 9966149 + 6011849 + 3962749\n",
    "number_events_QCD_Pt_1000toInf = 9638102 + 3990117\n",
    "number_events_WJetsToLNu_Pt_100To250 = 10088599 + 9944879\n",
    "number_events_WJetsToLNu_Pt_250To400 = 10021205 + 1001250 + 1000132\n",
    "number_events_WJetsToLNu_Pt_400To600 = 988234 + 951713\n",
    "number_events_WJetsToLNu_Pt_600ToInf = 985127 + 989482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacao dos eventos de BACKGROUND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_TT = ( cross_section_TT * Luminosidade ) / number_events_TT\n",
    "norm_inclusive_WZ = ( cross_section_inclusive_WZ * Luminosidade ) / number_events_inclusive_WZ\n",
    "norm_inclusive_ZZ = ( cross_section_inclusive_ZZ * Luminosidade ) / number_events_inclusive_ZZ\n",
    "norm_inclusive_WW = ( cross_section_inclusive_WW * Luminosidade ) / number_events_inclusive_WW\n",
    "norm_ST_s_channel = ( cross_section_ST_s_channel * Luminosidade ) / number_events_ST_s_channel\n",
    "norm_ST_t_channel_top = ( cross_section_ST_t_channel_top * Luminosidade ) / number_events_ST_t_channel_top\n",
    "norm_ST_t_channel_antitop = ( cross_section_ST_t_channel_antitop * Luminosidade) / number_events_ST_t_channel_antitop\n",
    "norm_ST_tW_antitop = ( cross_section_ST_tW_antitop * Luminosidade ) / number_events_ST_tW_antitop\n",
    "norm_ST_tW_top = ( cross_section_ST_tW_top * Luminosidade) / number_events_ST_tW_top\n",
    "norm_QCD_Pt_170to300  = ( cross_section_QCD_Pt_170to300  * Luminosidade ) / ( number_events_QCD_Pt_170to300  )\n",
    "norm_QCD_Pt_300to470  = ( cross_section_QCD_Pt_300to470  * Luminosidade ) / ( number_events_QCD_Pt_300to470  )\n",
    "norm_QCD_Pt_470to600  = ( cross_section_QCD_Pt_470to600  * Luminosidade ) / ( number_events_QCD_Pt_470to600  )\n",
    "norm_QCD_Pt_600to800  = ( cross_section_QCD_Pt_600to800  * Luminosidade ) / ( number_events_QCD_Pt_600to800  )\n",
    "norm_QCD_Pt_800to1000 = ( cross_section_QCD_Pt_800to1000 * Luminosidade ) / ( number_events_QCD_Pt_800to1000 )\n",
    "norm_QCD_Pt_1000toInf = ( cross_section_QCD_Pt_1000toInf * Luminosidade ) / ( number_events_QCD_Pt_1000toInf )\n",
    "norm_DYJetsToLL_Pt_100To250 = ( cross_section_DYJetsToLL_Pt_100To250 * Luminosidade ) / ( number_events_DYJetsToLL_Pt_100To250 )\n",
    "norm_DYJetsToLL_Pt_250To400 = ( cross_section_DYJetsToLL_Pt_250To400 * Luminosidade ) / ( number_events_DYJetsToLL_Pt_250To400 )\n",
    "norm_DYJetsToLL_Pt_400To650 = ( cross_section_DYJetsToLL_Pt_400To650 * Luminosidade ) / ( number_events_DYJetsToLL_Pt_400To650 )\n",
    "norm_DYJetsToLL_Pt_650ToInf = ( cross_section_DYJetsToLL_Pt_650ToInf * Luminosidade ) / ( number_events_DYJetsToLL_Pt_650ToInf )\n",
    "\n",
    "# Miguel, fazer a norm do W+jatos\n",
    "\n",
    "norm_WJetsToLNu_Pt_100To250 = ( cross_section_WJetsToLNu_Pt_100To250 * Luminosidade ) / ( number_events_WJetsToLNu_Pt_100To250 )\n",
    "norm_WJetsToLNu_Pt_250To400 = ( cross_section_WJetsToLNu_Pt_250To400 * Luminosidade ) / ( number_events_WJetsToLNu_Pt_250To400 )\n",
    "norm_WJetsToLNu_Pt_400To600 = ( cross_section_WJetsToLNu_Pt_400To600 * Luminosidade ) / ( number_events_WJetsToLNu_Pt_400To600 )\n",
    "norm_WJetsToLNu_Pt_600ToInf = ( cross_section_WJetsToLNu_Pt_600ToInf * Luminosidade ) / ( number_events_WJetsToLNu_Pt_600ToInf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Importando as amostras de Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DRELL YAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pt entre 100 e 250 GeV\n",
    "DY_100_250_2 = PATH + 'output-DY_100_250_2.h5'\n",
    "DY_100_250_3 = PATH + 'output-DY_100_250_3.h5'\n",
    "DY_100_250_4 = PATH + 'output-DY_100_250_4.h5'\n",
    "\n",
    "DY_100_250 = np.concatenate(  ( open_file( DY_100_250_2 ), \n",
    "                                open_file( DY_100_250_3 ), \n",
    "                                open_file( DY_100_250_4 ) )  , axis = 0 ) \n",
    "\n",
    "weight_DY_100_250 = np.array( [ norm_DYJetsToLL_Pt_100To250 ]*len( DY_100_250 ) ).reshape(-1,1)\n",
    "\n",
    "DY_100_250 = np.concatenate( ( DY_100_250 , weight_DY_100_250 ) , axis = 1 )\n",
    "\n",
    "# Miguel, fazer o caso acima para todos os arquivos, tanto back quanto signal\n",
    "\n",
    "# Pt entre 250 e 400 GeV\n",
    "DY_250_400_2 = PATH + 'output-DY_250_400_2.h5'\n",
    "DY_250_400_3 = PATH + 'output-DY_250_400_3.h5'\n",
    "DY_250_400_4 = PATH + 'output-DY_250_400_4.h5'\n",
    "\n",
    "DY_250_400 = np.concatenate(  ( open_file( DY_250_400_2 ), \n",
    "                                open_file( DY_250_400_3 ), \n",
    "                                open_file( DY_250_400_4 ) )  , axis = 0 ) \n",
    "\n",
    "weight_DY_250_400 = np.array( [ norm_DYJetsToLL_Pt_250To400 ]*len( DY_250_400 ) ).reshape(-1,1)\n",
    "\n",
    "DY_250_400 = np.concatenate( ( DY_250_400 , weight_DY_250_400 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 400 e 650 GeV\n",
    "DY_400_650_1 = PATH + 'output-DY_400_650_1.h5'\n",
    "DY_400_650_2 = PATH + 'output-DY_400_650_2.h5'\n",
    "DY_400_650_3 = PATH + 'output-DY_400_650_3.h5'\n",
    "\n",
    "DY_400_650 = np.concatenate(  ( open_file( DY_400_650_1 ), \n",
    "                                open_file( DY_400_650_2 ), \n",
    "                                open_file( DY_400_650_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_DY_400_650 = np.array( [ norm_DYJetsToLL_Pt_400To650 ]*len( DY_400_650 ) ).reshape(-1,1)\n",
    "\n",
    "DY_400_650 = np.concatenate( ( DY_400_650 , weight_DY_400_650 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 650 e infinito GeV\n",
    "DY_650_INF_1 = PATH + 'output-DY_650_INF_1.h5'\n",
    "DY_650_INF_2 = PATH + 'output-DY_650_INF_2.h5'\n",
    "\n",
    "DY_650_INF = np.concatenate(  ( open_file( DY_650_INF_1 ), \n",
    "                                open_file( DY_650_INF_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_DY_650_INF = np.array( [ norm_DYJetsToLL_Pt_650ToInf ]*len( DY_650_INF ) ).reshape(-1,1)\n",
    "\n",
    "DY_650_INF = np.concatenate( ( DY_650_INF , weight_DY_650_INF ) , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* QCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pt entre 170 e 300 GeV\n",
    "QCD_170_300_1 = PATH + 'output-QCD_170_300_1.h5'\n",
    "QCD_170_300_3 = PATH + 'output-QCD_170_300_3.h5'\n",
    "\n",
    "QCD_170_300 = np.concatenate(  ( open_file( QCD_170_300_1 ), \n",
    "                                 open_file( QCD_170_300_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_170_300 = np.array( [ norm_QCD_Pt_170to300 ]*len( QCD_170_300 ) ).reshape(-1,1)\n",
    "\n",
    "QCD_170_300 = np.concatenate( ( QCD_170_300 , weight_QCD_170_300 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 300 e 470 GeV\n",
    "QCD_300_470_1 = PATH + 'output-QCD_300_470_1.h5'\n",
    "QCD_300_470_2 = PATH + 'output-QCD_300_470_2.h5'\n",
    "QCD_300_470_3 = PATH + 'output-QCD_300_470_3.h5'\n",
    "\n",
    "QCD_300_470 = np.concatenate(  ( open_file( QCD_300_470_1 ), \n",
    "                                 open_file( QCD_300_470_2 ),\n",
    "                                 open_file( QCD_300_470_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_300_470 = np.array( [ norm_QCD_Pt_300to470 ]*len( QCD_300_470 ) ).reshape(-1,1)\n",
    "\n",
    "QCD_300_470 = np.concatenate( ( QCD_300_470 , weight_QCD_300_470 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 470 e 600 GeV\n",
    "QCD_470_600_1 = PATH + 'output-QCD_470_600_1.h5'\n",
    "QCD_470_600_2 = PATH + 'output-QCD_470_600_2.h5'\n",
    "QCD_470_600_3 = PATH + 'output-QCD_470_600_3.h5'\n",
    "\n",
    "QCD_470_600 = np.concatenate(  ( open_file( QCD_470_600_1 ), \n",
    "                                 open_file( QCD_470_600_2 ),\n",
    "                                 open_file( QCD_470_600_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_470_600 = np.array( [ norm_QCD_Pt_470to600 ]*len( QCD_470_600 ) ).reshape(-1,1)\n",
    "\n",
    "QCD_470_600 = np.concatenate( ( QCD_470_600 , weight_QCD_470_600 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 600 e 800 GeV\n",
    "QCD_600_800_1 = PATH + 'output-QCD_600_800_1.h5'\n",
    "QCD_600_800_2 = PATH + 'output-QCD_600_800_2.h5'\n",
    "QCD_600_800_3 = PATH + 'output-QCD_600_800_3.h5'\n",
    "\n",
    "QCD_600_800 = np.concatenate(  ( open_file( QCD_600_800_1 ), \n",
    "                                 open_file( QCD_600_800_2 ),\n",
    "                                 open_file( QCD_600_800_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_600_800 = np.array( [ norm_QCD_Pt_600to800 ]*len( QCD_600_800 ) ).reshape(-1,1)\n",
    "\n",
    "QCD_600_800 = np.concatenate( ( QCD_600_800 , weight_QCD_600_800 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 800 e 1000 GeV\n",
    "QCD_800_1000_1 = PATH + 'output-QCD_800_1000_1.h5'\n",
    "QCD_800_1000_2 = PATH + 'output-QCD_800_1000_2.h5'\n",
    "QCD_800_1000_3 = PATH + 'output-QCD_800_1000_3.h5'\n",
    "\n",
    "QCD_800_1000 = np.concatenate(  ( open_file( QCD_800_1000_1 ), \n",
    "                                  open_file( QCD_800_1000_2 ),\n",
    "                                  open_file( QCD_800_1000_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_800_1000 = np.array( [ norm_QCD_Pt_800to1000 ]*len( QCD_800_1000 ) ).reshape(-1,1)\n",
    "\n",
    "QCD_800_1000 = np.concatenate( ( QCD_800_1000 , weight_QCD_800_1000 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 1000 e infinito GeV\n",
    "QCD_1000_inf_1 = PATH +'output-QCD_1000_inf_1.h5'\n",
    "QCD_1000_inf_2 = PATH +'output-QCD_1000_inf_2.h5'\n",
    "\n",
    "QCD_1000_inf = np.concatenate(  ( open_file( QCD_1000_inf_1 ), \n",
    "                                  open_file( QCD_1000_inf_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_QCD_1000_inf = np.array( [ norm_QCD_Pt_1000toInf ]*len( QCD_1000_inf ) ).reshape(-1,1)\n",
    "\n",
    "QCD_1000_inf = np.concatenate( ( QCD_1000_inf , weight_QCD_1000_inf ) , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SINGLE ANTI-TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Antitop = PATH + 'output-Single_Antitop.h5'\n",
    "file_Single_Antitop = open_file(Single_Antitop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SINGLE ANTI-TOP tCHANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Antitop_tChannel = PATH + 'output-Single_Antitop_tChannel.h5'\n",
    "file_Single_Antitop_tChannel = open_file(Single_Antitop_tChannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SINGLE TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Top = PATH + 'output-Single_Top.h5'\n",
    "file_Single_Top = open_file(Single_Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SINGLE TOP sCHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Top_sChanel = PATH + 'output-Single_Top_sChanel.h5'\n",
    "file_Single_Top_sChanel = open_file(Single_Top_sChanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SINGLE TOP tCHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Top_tChannel = PATH + 'output-Single_Top_tChannel.h5'\n",
    "file_Single_Top_tChannel = open_file(Single_Top_tChannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TTBAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTbar = PATH + 'output-ttbar.h5'\n",
    "file_TTbar = open_file(TTbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W + JETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pt entre 100 e 250 GeV\n",
    "WJets_100_250_2 = PATH + 'output-WJets_100_250_2.h5'\n",
    "WJets_100_250_3 = PATH + 'output-WJets_100_250_3.h5'\n",
    "\n",
    "WJets_100_250 = np.concatenate(  ( open_file( WJets_100_250_2 ), \n",
    "                                   open_file( WJets_100_250_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_WJets_100_250 = np.array( [ norm_WJetsToLNu_Pt_100To250 ]*len( WJets_100_250 ) ).reshape(-1,1)\n",
    "\n",
    "WJets_100_250 = np.concatenate( ( WJets_100_250 , weight_WJets_100_250 ) , axis = 1 )\n",
    "\n",
    "\n",
    "# Pt entre 250 e 400 GeV \n",
    "WJets_250_400_2 = PATH +'output-WJets_250_400_2.h5'\n",
    "WJets_250_400_3 = PATH + 'output-WJets_250_400_3.h5'\n",
    "\n",
    "WJets_250_400 = np.concatenate(  ( open_file( WJets_250_400_2 ), \n",
    "                                   open_file( WJets_250_400_3 ) )  , axis = 0 ) \n",
    "\n",
    "weight_WJets_250_400 = np.array( [ norm_WJetsToLNu_Pt_250To400 ]*len( WJets_250_400 ) ).reshape(-1,1)\n",
    "\n",
    "WJets_250_400 = np.concatenate( ( WJets_250_400 , weight_WJets_250_400 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 400 e 600 GeV\n",
    "WJets_400_600_1 = PATH + 'output-WJets_400_600_1.h5'\n",
    "WJets_400_600_2 = PATH + 'output-WJets_400_600_2.h5'\n",
    "\n",
    "WJets_400_600 = np.concatenate(  ( open_file( WJets_400_600_1 ), \n",
    "                                   open_file( WJets_400_600_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_WJets_400_600 = np.array( [ norm_WJetsToLNu_Pt_400To600 ]*len( WJets_400_600 ) ).reshape(-1,1)\n",
    "\n",
    "WJets_400_600 = np.concatenate( ( WJets_400_600 , weight_WJets_400_600 ) , axis = 1 )\n",
    "\n",
    "# Pt entre 600 e infinito GeV\n",
    "WJets_600_inf_1 = PATH + 'output-WJets_600_inf_1.h5'\n",
    "WJets_600_inf_2 = PATH + 'output-WJets_600_inf_2.h5'\n",
    "\n",
    "WJets_600_inf = np.concatenate(  ( open_file( WJets_600_inf_1 ), \n",
    "                                   open_file( WJets_600_inf_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_WJets_600_inf = np.array( [ norm_WJetsToLNu_Pt_600ToInf ]*len( WJets_600_inf ) ).reshape(-1,1)\n",
    "\n",
    "WJets_600_inf = np.concatenate( ( WJets_600_inf , weight_WJets_600_inf ) , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* INCLUSIVO WW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "WW_1 = PATH + 'output-WW1.h5'\n",
    "WW_2 = PATH + 'output-WW2.h5'\n",
    "\n",
    "WW = np.concatenate(  ( open_file( WW_1 ), \n",
    "                        open_file( WW_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_WW = np.array( [ norm_inclusive_WW ]*len( WW ) ).reshape(-1,1)\n",
    "\n",
    "WW = np.concatenate( ( WW , weight_WW ) , axis = 1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* INCLUSIVO WZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "WZ = PATH + 'output-WZ.h5'\n",
    "file_WZ = open_file(WZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* INCLUSIVO ZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ_1 = PATH + 'output-ZZ1.h5'\n",
    "ZZ_2 = PATH + 'output-ZZ2.h5'\n",
    "\n",
    "ZZ = np.concatenate(  ( open_file( ZZ_1 ), \n",
    "                        open_file( ZZ_2 ) )  , axis = 0 ) \n",
    "\n",
    "weight_ZZ = np.array( [ norm_inclusive_ZZ ]*len( ZZ ) ).reshape(-1,1)\n",
    "\n",
    "ZZ = np.concatenate( ( ZZ , weight_ZZ ) , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numeração das colunas do numpy array\n",
    "\n",
    "0  --> massa do WW [:,0]\n",
    "\n",
    "1  --> Pt do W leptônico [:,1]\n",
    "\n",
    "2  --> DeltaPhi entre W_hadrônico e W_leptônico \n",
    "\n",
    "3  --> DeltaPhi entre Jatos e o MET \n",
    "\n",
    "4  --> jetAK8_pt \n",
    "\n",
    "5  --> jetAK8_eta\n",
    "\n",
    "6  --> jetAK8_prunedMass\n",
    "\n",
    "7  --> jetAK8_tau21\n",
    "\n",
    "8  --> METPt\n",
    "\n",
    "9  --> muon_pt\n",
    "\n",
    "10 --> muon_eta\n",
    "\n",
    "11 --> ExtraTracks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
